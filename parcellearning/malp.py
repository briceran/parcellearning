#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Tue May  9 18:17:38 2017

@author: kristianeschenburg

"""

import classifierUtilities as cu
import loaded as ld

import copy
import h5py
import inspect
import os
import pickle

from joblib import Parallel, delayed
import multiprocessing

import numpy as np
from sklearn import ensemble
from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier

# number of cores to parallelize over
NUM_CORES = multiprocessing.cpu_count()

class Atlas(object):
    
    """
    Class to instantiate a classifier for a single training set.
    
    Parameters:
    - - - - -
        depth : maximum depth of grown trees
        
        n_estimators : number of estimating trees per forest
        
        power : power with which to weight the matching frequency
    """
    
    def __init__(self,depth=5,n_estimators=60,power=1):
        
        self.classifier = ensemble.RandomForestClassifier(n_estimators=n_estimators,
                                                          max_depth=depth,n_jobs=-1)
        
        self.power = power

    def set_params(self,**params):
        
        """
        Update parameters with user-specified dictionary.
        """
        
        args,_,_,_ = inspect.getargspec(self.__init__)
        
        if params:
            for key in params:
                if key in args:
                    setattr(self,key,params[key])
            
    def fit(self,x_train,y_train,neighbors,labels,model_type='ori',**kwargs):
        
        """
        Method to initialize training data and fit the classifiers.
        
        Parameters:
        - - - - -
            x_train : training data in dictionary format, where keys are 
                        label values and values are data arrays corresponding
                        to vertices assigned to those labels
                        
            y_train : training labels in dictionary format, where keys are
                        label values and values are vectors with the same
                        number of samples as x_train[label]

            neighbors : label-label neighborhood information
            
            labels : list of classes in training set
            
            model_type : intra-classifier class comparisons

            kwargs : optional arguments for classifier
        """
        
        self.labels = labels
        self.neighbors = neighbors
        
        classifier = self.classifier

        labelData = x_train
        response = y_train

        # get valid arguments for supplied classifier
        # get valid parameters passed by user
        # update classifier parameters
        # save base models
        classifier_params = inspect.getargspec(classifier.__init__)
        classArgs = cu.parseKwargs(classifier_params,kwargs)
        classifier.set_params(**classArgs)
        self.classifier = classifier
        
        print 'depth: {}'.format(classifier.max_depth)
        print 'nEst: {}'.format(classifier.n_estimators)
            
        model_selector = {'oVo': OneVsOneClassifier(classifier),
                          'oVr': OneVsRestClassifier(classifier),
                          'ori': classifier}
        models = {}

        for i,l in enumerate(labels):
            if l in labelData.keys() and l in neighbors.keys():

                # copy the model (due to passing by reference)
                models[l] = copy.deepcopy(model_selector[model_type])

                # get associated neighboring regions (from adjacency or confusion data)
                nbs = neighbors[l]

                if isinstance(nbs,list):
                    labelNeighbors = list(nbs)
                else:
                    nbs = int(nbs)
                    labelNeighbors = [nbs]
                    
                labelNeighbors.append(l)
                labelNeighbors = list(set(labelNeighbors).intersection(labels))
                
                # build classifier training data upon request
                [learned,y] = cu.mergeLabelData(labelData,response,labelNeighbors)
    
                models[l].fit(learned,np.squeeze(y))

        self.models = models
        self.fitted = True

    def predict(self,x_test,mm,ltvm,softmax_type = 'FORESTS',**kwargs):
        
        """
        Method to predict labels of test data.
        
        Parameters:
        - - - - -
            x_test : test data in array format, as generated by the 
                        PrepareObject.processTesting()
            
            mm : matching matrix in either binary format, or frequency format.
                    If there are V test samples and L labels in the training
                    data, the mm is VxL, where each index is {0,1} or [0,1].
                    
            ltvm : label to vertex map as a dictionary.  Keys are label values
                    and values are lists of vertices that map to that label.
                    
            softmax_type : type of neighborhood constraint to apply ('BASE',
                            'TREES','FORESTS')
            
            kwargs : optional arguments to apply to prediction step                    
        """

        if kwargs:
            if 'power' in kwargs.keys():
                p = kwargs['power']
            else:
                p = 1
        else:
            p = 1

        funcs = {'BASE': baseSoftMax,
                 'TREES': treeSoftMax,
                 'FORESTS': forestSoftMax}
        
        labels = self.labels
        neighbors = self.neighbors
        
        R = len(labels)
        [xTest,yTest] = x_test.shape

        # initialize prediction dictionary
        baseline = np.zeros((x_test.shape[0],R+1))

        mm = np.power(mm,p)

        for lab in labels:
            if lab in neighbors.keys():
                
                members = ltvm[lab]
                memberData = x_test[members,:]
                estimator = self.models[lab]
                
                if len(members) > 0:
                    preds = funcs[softmax_type](estimator,members,memberData,mm,R)
                    baseline = cu.updatePredictions(baseline,members,preds)
                
        predicted = np.argmax(baseline,axis=1)
        
        self.baseline = baseline
        self.predicted = predicted
        
        return [baseline,predicted]

def baseSoftMax(metaEstimator,members,memberData,mm,R):
    
    """
    """

    predicted = np.squeeze(metaEstimator.predict(memberData))

    return predicted
        

def forestSoftMax(metaEstimator,members,memberData,mm,R):
    
    """
    Method to restrict whole random decision forest soft-max prediction to
    labels generated in the surface registration step.
    
    Parameters:
    - - - - -
        
        metaEstimator : single decision forest
        members : current vertices of interest
        memberData : feature data for members
        mm : binary matching matrix
        R : number of labels in training set
    """

    memberMatrix = mm[members,:]
    predProbs = metaEstimator.predict_proba(memberData)
    
    classes = metaEstimator.classes_
    forestMatrix = np.zeros((len(members),mm.shape[1]+1))
    forestMatrix[:,classes] = predProbs
    forestMatrix = forestMatrix[:,1:]
    
    predThresh = memberMatrix*(1.*forestMatrix)
    
    labels = np.argmax(predThresh,axis=1)+1

    return labels


def treeSoftMax(metaEstimator,members,memberData,mm,R):
    
    """
    Super method for classification prediction with tree-level restriction.
    
    Parameters:
    - - - - -
        metaEstimator : single decision forest
                
        members : current vertices of interest
        
        memberData : feature data for members
        
        mm : binary matching matrix
        
        R : number of labels in training set
        
    A single metaEstimator consists of a set of sub-estimators.  The classes 
    for the metaEstimator corresponds to a list of K values to predict 
    i.e. [1,5,6,7,100]
    
    Each sub-estimator, however, has 0-K indexed classes (i.e. [0,1,2,3,4]), 
    where the index corresponds to the position in the metaEstimator 
    class list.
    """

    memberMatrix = mm[members,:]

    treeProbs = []
    classes = []
    for est in metaEstimator.estimators_:
        classes.append(metaEstimator.classes_)
        treeProbs.append(est.predict_proba(memberData))

    predictedLabels = []
    
    treeMatrix = np.zeros((len(members),mm.shape[1]+1))
        
    for k in np.arange(len(treeProbs)):
        
        treeClasses = np.squeeze(classes[k]).astype(np.int32)
        treeProbabilities = treeProbs[k]
        
        tm = copy.deepcopy(treeMatrix)
        tm[:,treeClasses] = treeProbabilities
        tm = tm[:,1:]
        
        treeThresh = memberMatrix*(1.*tm)
        predictedLabels.append(np.argmax(treeThresh,axis=1)+1)
    
    predictedLabels = np.column_stack(predictedLabels)

    classification = []
        
    for i in np.arange(predictedLabels.shape[0]):
        
        L = list(predictedLabels[i,:])
        maxProb = max(set(L),key=L.count)
        classification.append(maxProb)
    
    classification = np.asarray(classification)
        
    return classification

class MultiAtlas(object):
    
    """
    Class to perform multi-atlas classification based on the combine results
    of single / multi-subject classifiers.
    
    Parameters:
    - - - - -
        feats : features to include in the models
        
        threshold : threhsold to apply to mappings
        
        scale : standardize the training data
    """
    
    def __init__(self,atlas_size = 1,atlases=None):
        
        """
        Method to initialize Mutli-Atlas label propagation scheme.
        
        Parameters:
        - - - - -
            atlas_size : number of subjects per atlas

            atlases : number of atlases to generate

        """
        
        if atlas_size < 1:
            raise ValueError('Before initializing training data, atlas_size '\
                             'must be at least 1.')
        
        if atlases is not None and atlases < 0:
            raise ValueError('atlases must be positive integer or None.')

        self.atlas_size = atlas_size
        self.atlases = atlases
        
    def set_params(self,**params):
        
        """
        Update parameters with user-specified dictionary.
        """
        
        args,_,_,_ = inspect.getargspec(self.__init__)
        
        if params:
            for key in params:
                if key in args:
                    setattr(self,key,params[key])


    def load(self,trainingData,trainingLabels,**kwargs):
        
        """
        Method to initialize training data for MALP framework.
        
        Parameters:
        - - - - -
            trainObject : dictionary of training data, where keys are subjects
                            and values are data arrays
        Returns:
        - - - -
            dataSets : list of dictionaries corresponding to data for each 
                        atlas
            labelSets : list of dictionaries, corresponding to label data
                        for each atlas
        """
        
        assert isinstance(trainingData,dict)
        assert isinstance(trainingLabels,dict)
        assert trainingData.keys() == trainingLabels.keys()

        subjects = trainingData.keys()

        if not self.atlases:
            self.atlases = len(subjects)
        else:
            self.atlases = min(self.atlases,len(subjects))

        dataSets = []
        labelSets = []

        # If the number of atlases is 1, all subject data will be aggregated
        # into a single dictionary
        if self.atlas_size == 1:
            subjectSet = np.random.choice(subjects,size=self.atlases,
                                          replace=False)

            for s in subjectSet:
                td = {s: trainingData[s]}
                tl = {s: trainingLabels[s]}
                
                dataSets.append(td)
                labelSets.append(tl)
        
        # Otherwise, N = self.atlases will be created, each with 
        # K = self.atlas_size (possibly overlapping) training subjects in it
        # This will be N separate training dictionaries datasets.
        else:
            size = self.atlas_size
            chunks = len(subjects) / size
            np.random.shuffle(subjects)
            
            # Make self.atlases independent atlases
            if chunks >= self.atlases:                
                for j in np.arange(self.atlases):
                    subjectSet = subjects[j*size:(j+1)*size]
                    
                    td = {s: trainingData[s] for s in subjectSet}
                    tl = {s: trainingLabels[s] for s in subjectSet}
                    
                    dataSets.append(td)
                    labelSets.append(tl)
                    
            # Make as many independent atlases as possible, then select subjects
            # randomly until self.atlases have been created
            else:
                
                rem = self.atlases-chunks
                for j in np.arange(chunks):
                    subjectSet = subjects[j*size:(j+1)*size]
                    
                    td = {s: trainingData[s] for s in subjectSet}
                    tl = {s: trainingLabels[s] for s in subjectSet}
                    
                    dataSets.append(td)
                    labelSets.append(tl)
                    
                for j in np.arange(rem):
                    
                    subjectSet = np.random.choice(subjects,size=size,replace=False)
                    
                    td = {s: trainingData[s] for s in subjectSet}
                    tl = {s: trainingLabels[s] for s in subjectSet}
                    
                    dataSets.append(td)
                    labelSets.append(tl)

        return [dataSets,labelSets]
        

def parallelFitting(trainingData,trainingLabels,maps,**fit_params):

    """
    Method to fit a set of Atlas objects.
    
    Parameters:
    - - - - -
        multiAtlas : object containing independent datasets
        maps : label neighborhood map
        features : features to include in model
    """

    BaseAtlas = Atlas()
    BaseAtlas.set_params(fit_params)

    # fit atlas on each componentraransarra
    fittedAtlases = Parallel(n_jobs=NUM_CORES)(delayed(atlasFit)(BaseAtlas,dat,
                             labs,maps,**fit_params) for (dat,labs) in zip(trainingData,trainingLabels))
    
    return fittedAtlases
    
def atlasFit(baseAtlas,x_train,y_train,maps,**kwargs):
    
    """
    Single model fitting step.
    """

    labelSet = np.arange(1,181)
    atlasData = cu.mapLabelsToData(x_train,y_train,labelSet)
    atlasLabs = cu.buildResponseVector(labelSet,atlasData)
    
    atl = copy.deepcopy(baseAtlas)
    atl.fit(atlasData,atlasLabs,maps,labelSet,model_type='ori',**kwargs)
    
    return atl


def parallelPredicting(models,x_test,matches,ltvm,**kwargs):
    
    """
    Method to predicted test labels in parallel
    """
    
    predictedLabels = Parallel(n_jobs=NUM_CORES)(delayed(atlasPredict)(models[i],
                               x_test,matches,ltvm,**kwargs) for i,m in enumerate(models))

    predictedLabels = np.column_stack(predictedLabels)

    classification = []
        
    for i in np.arange(predictedLabels.shape[0]):
        
        L = list(predictedLabels[i,:])
        maxProb = max(set(L),key=L.count)
        classification.append(maxProb)
    
    classification = np.asarray(classification)
    
    return predictedLabels

def atlasPredict(mod,x_test,matches,ltvm,**kwargs):
    
    """
    Single model prediction step.
    """
    
    #args,_,_,_ = inspect.getargspec(model.__init__)
    #modelArgs = cu.parseKwargs(args,kwargs)
    #mod.set_params(**modelArgs)
    
    [baseline,predicted] = mod.predict(x_test,matches,ltvm,softmax_type='FORESTS',**kwargs)

    return predicted

